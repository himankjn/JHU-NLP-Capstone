paste(unlist(str_split("this means HELLO last word"," "))[-1])
unlist(str_split("this means HELLO last word"," "))[-1]
paste(unlist(str_split("this means HELLO last word"," "))[-1],sep=' ',collapse = TRUE)
paste(unlist(str_split("this means HELLO last word"," "))[-1],sep=' ',collapse = 'a')
paste(unlist(str_split("this means HELLO last word"," "))[-1],collapse=' ')
removelastword<-function(sent){
return(paste(unlist(str_split(sent," "))[-1],collapse=' '))
}
removelastword("what are u doing")
removelastword<-function(sent){
return(paste(unlist(str_split(sent," "))[-1],collapse=' '))
}
bigram <- function(words){
out<-as.character(filter(bidf,base==words)$prediction[1])
ifelse(out =="NA", "?", return(out))
}
trigram <- function(words){
out<-as.character(filter(tridf,base==words)$prediction[1])
ifelse(out=="NA", bigram(removelastword(words)), return(out))
}
tetragram <- function(words){
out<-as.character(filter(tetradf,base==words)$prediction[1])
ifelse(out=="NA", trigram(removelastword(words)), return(out))
}
pentagram<-function(words){
out<-as.character(filter(pentadf,base==words)$prediction[1])
ifelse(out=="NA", tetragram(removelastword(words)), return(out))
}
ngrams <- function(input){
#remove url
removeURL<-function(x) gsub("https?.*.com ",'',x)
#remove anything other than alphabets and spaces:
removenonalpha<-function(x) gsub("[^[:alpha:][:space:]]",'',x)
input<-removenonalpha(input)
input<-removeURL(input)
count<-str_count(input,boundary("word"))
words <- tolower(input)
# Call the matching functions
out <- ifelse(count == 1, bigram(words),
ifelse (count == 2, trigram(words),
ifelse (count == 3, tetragram(words),
pentagram(words))))
return(out)
}
ngrams("what adf zuik damak")
ngrams("what adf zuik")
ngrams("what are you")
ngrams("what are zukai you")
input<-"what are zukai you"
input<-removenonalpha(input)
input<-removeURL(input)
count<-str_count(input,boundary("word"))
words <- tolower(input)
words
count
words<-'what are zukai you'
(filter(pentadf,base==words)$prediction[1])
as.character(filter(pentadf,base==words)$prediction[1])
as.character(filter(pentadf,base==words)$prediction[1])==NA
as.character(filter(pentadf,base==words)$prediction[1])=="NA"
as.character(filter(pentadf,base==words)$prediction[1])
is.na(as.character(filter(pentadf,base==words)$prediction[1]))
bigram <- function(words){
out<-as.character(filter(bidf,base==words)$prediction[1])
ifelse(out =="NA", "?", return(out))
}
trigram <- function(words){
out<-as.character(filter(tridf,base==words)$prediction[1])
ifelse(is.na(out), bigram(removelastword(words)), return(out))
}
tetragram <- function(words){
out<-as.character(filter(tetradf,base==words)$prediction[1])
ifelse(is.na(out), trigram(removelastword(words)), return(out))
}
pentagram<-function(words){
out<-as.character(filter(pentadf,base==words)$prediction[1])
ifelse(is.na(out),tetragram(removelastword(words)), return(out))
}
ngrams <- function(input){
#remove url
removeURL<-function(x) gsub("https?.*.com ",'',x)
#remove anything other than alphabets and spaces:
removenonalpha<-function(x) gsub("[^[:alpha:][:space:]]",'',x)
input<-removenonalpha(input)
input<-removeURL(input)
count<-str_count(input,boundary("word"))
words <- tolower(input)
# Call the matching functions
out <- ifelse(count == 1, bigram(words),
ifelse (count == 2, trigram(words),
ifelse (count == 3, tetragram(words),
pentagram(words))))
return(out)
}
ngrams("what are zukai you")
ngrams("you")
ngrams("still struggling but the")
ngrams("romantic date at the")
ngrams("date at the")
ngrams("at the")
ngrams("off and be on my")
ngrams("and be on my")
ngrams("be on my")
ngrams("and be on my")
pentadf
pentadf$base
pentadf$base=="off and be on my"
pentadf[pentadf$base=="off and be on my"]
pentadf[pentadf$base=="off and be on my",]
pentadf[pentadf$base=="and be on my",]
ngrams("eyes with his little")
ngrams(" I'd live and I'd")
ngrams("I'd live and I'd")
ngrams("tgh akf I'd live and I'd")
?select
sent="tgh afk I'd live and I'd"
paste(unlist(str_split(sent," "))[-1],collapse=' ')
paste(unlist(str_split(sent," ")),collapse=' ')
unlist(str_split(sent," "))
tail(unlist(str_split(sent," ")))
tail(unlist(str_split(sent," ")),4)
paste(tail(unlist(str_split(sent," ")),4),collapse=" ")
pentagram<-function(words){
words<-paste(tail(unlist(str_split(sent," ")),4),collapse=" ")
out<-as.character(filter(pentadf,base==words)$prediction[1])
ifelse(is.na(out),tetragram(removelastword(words)), return(out))
}
ngrams("tgh akf I'd live and I'd")
ngrams("tgh akf I'd live and I'd")
removelastword<-function(sent){
return(paste(unlist(str_split(sent," "))[-1],collapse=' '))
}
bigram <- function(words){
out<-as.character(filter(bidf,base==words)$prediction[1])
ifelse(out =="NA", "?", return(out))
}
trigram <- function(words){
out<-as.character(filter(tridf,base==words)$prediction[1])
ifelse(is.na(out), bigram(removelastword(words)), return(out))
}
tetragram <- function(words){
out<-as.character(filter(tetradf,base==words)$prediction[1])
ifelse(is.na(out), trigram(removelastword(words)), return(out))
}
pentagram<-function(words){
words<-paste(tail(unlist(str_split(sent," ")),4),collapse=" ")
out<-as.character(filter(pentadf,base==words)$prediction[1])
ifelse(is.na(out),tetragram(removelastword(words)), return(out))
}
ngrams <- function(input){
#remove url
removeURL<-function(x) gsub("https?.*.com ",'',x)
#remove anything other than alphabets and spaces:
removenonalpha<-function(x) gsub("[^[:alpha:][:space:]]",'',x)
input<-removenonalpha(input)
input<-removeURL(input)
count<-str_count(input,boundary("word"))
words <- tolower(input)
# Call the matching functions
out <- ifelse(count == 1, bigram(words),
ifelse (count == 2, trigram(words),
ifelse (count == 3, tetragram(words),
pentagram(words))))
return(out)
}
ngrams("tgh akf I'd live and I'd")
sent
words<-paste(tail(unlist(str_split(sent," ")),4),collapse=" ")
words
ngrams("tgh akf I'd live and I'd")
pentagram("tgh afk id live and id")
tetra("tgh afk id live and id")
tetragram("tgh afk id live and id")
tetragram("live and id")
pentagram("tgh afk id live and id")
pentagram("id live and id")
sent<-"id live and id"
words<-paste(tail(unlist(str_split(sent," ")),4),collapse=" ")
words
pentagram("id live and id")
pentagram("id live and id")
pentagram("akf id live and id")
pentagram("tgh akf id live and id")
ngram("tgh akf id live and id")
ngrams("tgh akf id live and id")
ngrams("what the adfna is still dfone")
ngrams("what the adfna afj are you")
ngrams("what the adfna afj is it")
ngrams("is this")
ngrams("so wif nothing who are")
bigram
bidf
bidf[bidf$freq==max(bidf$freq)]
ngrams("so wif nothing who is")
pentagram<-function(words){
words<-paste(tail(unlist(str_split(words," ")),4),collapse=" ")
out<-as.character(filter(pentadf,base==words)$prediction[1])
ifelse(is.na(out),tetragram(removelastword(words)), return(out))
}
ngrams <- function(input){
#remove url
removeURL<-function(x) gsub("https?.*.com ",'',x)
#remove anything other than alphabets and spaces:
removenonalpha<-function(x) gsub("[^[:alpha:][:space:]]",'',x)
input<-removenonalpha(input)
input<-removeURL(input)
count<-str_count(input,boundary("word"))
words <- tolower(input)
# Call the matching functions
out <- ifelse(count == 1, bigram(words),
ifelse (count == 2, trigram(words),
ifelse (count == 3, tetragram(words),
pentagram(words))))
return(out)
}
ngrams("so wif nothing who is")
ngrams("so asfis asdfj ash tf this is one")
shiny::runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
input$box1
ngrams("hello this")
ngrams("hello this")
library(stringi)
ngrams("hello this")
library(stringr)
ngrams("hello this")
ngrams("hello this")
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
ngrams("how have")
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
pentadf<-readRDS("C:/Users/himan/Desktop/cap/pentadf.rds")
#prediction
bidf<-readRDS("C:/Users/himan/Desktop/cap/bidf.rds")
saveRDS(bidf,file="C:/Users/himan/Desktop/cap/bidf.rds")
saveRDS(tridf,file="C:/Users/himan/Desktop/cap/tridf.rds")
saveRDS(tetradf,file="C:/Users/himan/Desktop/cap/tetradf.rds")
saveRDS(pentadf,file="C:/Users/himan/Desktop/cap/pentadf.rds")
#prediction
bidf<-readRDS("C:/Users/himan/Desktop/cap/bidf.rds")
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
input<-stripWhitespace(input)
library(quanteda)
input<-stripWhitespace(input)
count<-str_count(input,boundary("word"))
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
library(quanteda)
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
stripWhitespace("hello ")
stripWhitespace("hello  ")
stripWhitespace("hello    ")
?stripWhitespace
trimws("hello ",whcihc="both")
trimws("hello ",which="both")
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
bidf
mutate(bidf,prop=freq/sum(freq))
mutate(bidf,prop=freq/sum(freq))%>%arrange(desc(prop))
mutate(bidf,prop=freq/sum(freq))%>%arrange(desc(prop))%>%mutate(coverage=cumsum(prop))%>%filter(coverage<=0.5)
bidf<-mutate(bidf,prop=freq/sum(freq))%>%arrange(desc(prop))%>%mutate(coverage=cumsum(prop))%>%filter(coverage<=0.5)
tridf<-mutate(tridf,prop=freq/sum(freq))%>%arrange(desc(prop))%>%mutate(coverage=cumsum(prop))%>%filter(coverage<=0.5)
tetradf<-mutate(tetradf,prop=freq/sum(freq))%>%arrange(desc(prop))%>%mutate(coverage=cumsum(prop))%>%filter(coverage<=0.5)
pentadf<-mutate(pentadf,prop=freq/sum(freq))%>%arrange(desc(prop))%>%mutate(coverage=cumsum(prop))%>%filter(coverage<=0.5)
saveRDS(bidf,"C:/Users/himan/Desktop/cap/bidf2.rds")
saveRDS(tridf,"C:/Users/himan/Desktop/cap/tridf2.rds")
saveRDS(tetradf,"C:/Users/himan/Desktop/cap/tetradf2.rds")
saveRDS(pentadf,"C:/Users/himan/Desktop/cap/pentadf2.rds")
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
saveRDS(pentadf,file="C:/Users/himan/Desktop/cap/pentadf.rds")
saveRDS(tetradf,file="C:/Users/himan/Desktop/cap/tetradf.rds")
saveRDS(tridf,file="C:/Users/himan/Desktop/cap/tridf.rds")
saveRDS(bidf,file="C:/Users/himan/Desktop/cap/bidf.rds")
bidf
object.size(bidf)
bidf[,c(base,prediction,freq)]
bidf[,c('base','prediction','freq')]
object.size(bidf)
object.size(bidf[,c('base','prediction','freq')])
bidf<-bidf[,c('base','prediction','freq')]
saveRDS(bidf,file="C:/Users/himan/Desktop/cap/bidf.rds")
tridf<-tridf[,c('base','prediction','freq')]
saveRDS(tridf,file="C:/Users/himan/Desktop/cap/tridf.rds")
tetradf<-tetradf[,c('base','prediction','freq')]
saveRDS(tetradf,file="C:/Users/himan/Desktop/cap/tetradf.rds")
pentadfdf<-pentadf[,c('base','prediction','freq')]
saveRDS(pentadf,file="C:/Users/himan/Desktop/cap/pentadf.rds")
object.size(bidf)
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
ngrams("afi")
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
?textOutput
?textOutput
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
?h2
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
object.size(pentadf)
object.size(pentadf[,c("base","prediction")])
bidf<-bidf[,c('base','prediction')]
saveRDS(bidf,file="C:/Users/himan/Desktop/cap/bidf.rds")
pentadfdf<-pentadf[,c('base','prediction')]
saveRDS(pentadf,file="C:/Users/himan/Desktop/cap/pentadf.rds")
tetradf<-tetradf[,c('base','prediction')]
saveRDS(tetradf,file="C:/Users/himan/Desktop/cap/tetradf.rds")
tridf<-tridf[,c('base','prediction')]
saveRDS(tridf,file="C:/Users/himan/Desktop/cap/tridf.rds")
bidf
pentadfdf<-pentadf[,c('base','prediction')]
pentadf<-pentadf[,c('base','prediction')]
pentadf<-mutate(pentadf,prop=freq/sum(freq))%>%arrange(desc(prop))%>%mutate(coverage=cumsum(prop))%>%filter(coverage<=0.5)
pentadf<-pentadf[,c('base','prediction')]
saveRDS(pentadf,file="C:/Users/himan/Desktop/cap/pentadf.rds")
rm(pentadfdf)
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
str_count("",boundary("word"))
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
shiny::runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
- Bullet 1
- Bullet 2
- Bullet 3
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
print(paste("corpus size:",object.size(mycorpus)/1024^2,'and token size:',object.size(toks)/1024^2))
shiny::runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
source("prediction.R")
source("C:/Users/himan/Desktop/cap/prediction.R")
runApp('C:/Users/himan/Desktop/cap/JHU_Capstone')
runApp('C:/Users/himan/Desktop/cap')
load('toks.rds')
load('toks.rds')
runApp('C:/Users/himan/Desktop/cap')
load('toks.rds')
setwd('C:/Users/himan/Desktop/cap/')
load('toks.rds')
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
sizeblogs<-file.size('final/en_US/en_US.blogs.txt')/1024^2
sizenews<-file.size('final/en_US/en_US.news.txt')/1024^2
sizetwitter<-file.size('final/en_US/en_US.twitter.txt')/1024^2
wordsblogs<-strsplit(system("wc -w final/en_US/en_US.blogs.txt",intern=TRUE),' ')[[1]][1]
system("find /c /v final/en_US/en_US.blogs.txt")
news
blogs
#reading few lines from files
news<-readLines("final/en_US/en_US.news.txt")
#reading few lines from files
news<-readLines("final/en_US/en_US.news.txt")
blogs<-readLines("final/en_US/en_US.blogs.txt")
blogs<-readLines("final/en_US/en_US.blogs.txt")
twitter<-readLines("final/en_US/en_US.twitter.txt")
nchars(blogs)
nchar(blogs)
sum(nchar(blogs))
sum(nchar(blogs,type="byte"))
sum(nchar(blogs,type="bytes"))
sum(nchar(blogs,type="width"))
length(blogs)
blogs
tail(blogs)
sapply(strsplit(news, " "), length)
sum(sapply(strsplit(news, " "), length))
file.size(blogs)
wc2<-wordcloud2(bitokdf[1:1000,])
wordcloud2(unidf[1:2000,])
# Required R libraries:
library(stringi)
library(ggplot2)
library(data.table)
library(htmlwidgets)
library(magrittr)
library(webshot)
#webshot::install_phantomjs()
library(markdown)
library(RWeka)
library(openNLP)
library(wordcloud2)
library(wordcloud)
library(tm)
library(NLP)
library(qdap)
library(devtools)
library(plotrix)
wordcloud2(unidf[1:2000,])
unidf<-data.table(table(unitok))
#reading few lines from files
news<-readLines("final/en_US/en_US.news.txt")
blogs<-readLines("final/en_US/en_US.blogs.txt")
twitter<-readLines("final/en_US/en_US.twitter.txt")
#sampling only 5000 lines from each file for performance efficiency:
fewnews<-sample(news,5000)
fewblogs<-sample(blogs,5000)
fewtwit<-sample(twitter,5000)
#combining text from the three files
fewtext<-paste(fewnews,fewblogs,fewtwit)
fewtext[[1]]
sizeblogs<-file.size('final/en_US/en_US.blogs.txt')/1024^2
sizenews<-file.size('final/en_US/en_US.news.txt')/1024^2
sizetwitter<-file.size('final/en_US/en_US.twitter.txt')/1024^2
wordsblogs<-sum(sapply(strsplit(blogs, " "), length))
#making corpus
corpus<-VCorpus(VectorSource(fewtext))
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# strip whitespaces left and right
corpus <- tm_map(corpus, stripWhitespace)
# convert all chars to lowercase
corpus <- tm_map(corpus, content_transformer(tolower))
# remove special characters
corpus <- tm_map(corpus, removePunctuation)
#getting rid profane words:
badwords<-file('badwords.txt')
badwords<-VectorSource(badwords)
corpus <- tm_map(corpus, removeWords, badwords)
corpus
corpus<-tm_map(corpus,stemDocument,language='english')
cleanData<-data.table(text=(sapply(corpus,'[','content')),stringsAsFactors = F)
unitok<-NGramTokenizer(cleanData,Weka_control(min=1,max=1))
bitok <- NGramTokenizer(cleanData, Weka_control(min = 2, max = 2, delimiters = " \\r\\n\\t.
,;:\"()?!"))
tritok <- NGramTokenizer(cleanData, Weka_control(min = 3, max = 3, delimiters = " \\r\\n\\t
.,;:\"()?!"))
head(unitok)
tetratok<-NGramTokenizer(cleanData,Weka_control(min=4,max=4,delimiters = " \\r\\n\\t
.,;:\"()?!"))
head(bitok)
head(tritok)
unidf<-data.table(table(unitok))
unidf<-unidf[order(-N),]
colnames(unidf)<-c('word','freq')
bitokdf<-data.table(table(bitok))
bitokdf<-bitokdf[order(-N),]
colnames(bitokdf)<-c('word','freq')
tritokdf<-data.table(table(tritok))
tritokdf<-tritokdf[order(-N),]
colnames(tritokdf)<-c('word','freq')
tetratokdf<-data.table(table(tetratok))
tetratokdf<-tetratokdf[order(-N),]
colnames(tetratokdf)<-c('word','freq')
bitokdf[1:15,]
tritokdf[1:15,]
wordcloud2(unidf[1:2000,])
splot<-ggplot(unidf[1:15],aes(x=word,y=freq))+geom_bar(stat='identity',fill='red',colour='blue')+
geom_text(aes(label=freq),vjust=-1)+labs(title='15 Most Frequent unigrams',x='1-Gram',y='Frequency')
splot
wc2<-wordcloud2(bitokdf[1:1000,])
saveWidget(wc2,"2.html",selfcontained = F)
webshot::webshot("2.html","2.png",vwidth = 1000, vheight = 800, delay =10)
bplot<-ggplot(bitokdf[1:15],aes(x=word,y=freq))+geom_bar(stat='identity',fill='blue',colour='red')+geom_text(aes(label=freq),vjust=-1)+
theme(axis.text.x=element_text(angle = 45,hjust=1))+labs(title='15 Most Frequent bigrams',x='2-Gram',y='Frequency')
bplot
webshot::install_phantomjs()
library(phantomjs)
wc2<-wordcloud2(bitokdf[1:1000,])
saveWidget(wc2,"2.html",selfcontained = F)
webshot::webshot("2.html","2.png",vwidth = 1000, vheight = 800, delay =10)
bplot<-ggplot(bitokdf[1:15],aes(x=word,y=freq))+geom_bar(stat='identity',fill='blue',colour='red')+geom_text(aes(label=freq),vjust=-1)+
theme(axis.text.x=element_text(angle = 45,hjust=1))+labs(title='15 Most Frequent bigrams',x='2-Gram',y='Frequency')
bplot
wc2<-wordcloud2(bitokdf[1:1000,])
saveWidget(wc2,"2.html",selfcontained = F)
webshot::webshot("2.html","2.png",vwidth = 1000, vheight = 800, delay =10)
bplot<-ggplot(bitokdf[1:15],aes(x=word,y=freq))+geom_bar(stat='identity',fill='blue',colour='red')+geom_text(aes(label=freq),vjust=-1)+
theme(axis.text.x=element_text(angle = 45,hjust=1))+labs(title='15 Most Frequent bigrams',x='2-Gram',y='Frequency')
bplot
webshot::webshot("2.html","2.png",vwidth = 1000, vheight = 800, delay =10)
wc2<-wordcloud2(bitokdf[1:1000,])
saveWidget(wc2,"2.html",selfcontained = F)
webshot::webshot("2.html","2.png",vwidth = 1000, vheight = 800, delay =10)
![second](2.png)
wc2<-wordcloud2(bitokdf[1:1000,])
saveWidget(wc2,"2.html",selfcontained = F)
webshot::webshot("2.html","2.png",vwidth = 1000, vheight = 800, delay =10)
